{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Intro to Web Scraping! \n", "\n", "![](https://media.giphy.com/media/1dcWGdOBg0wcU/giphy.gif)\n", "\n", "    \n", "### Today's Goals:\n", "1. Retrieve the HTML of a webpage with the `requests` library.\n", "2. Introduction to the `tree` structure of `HTML`.\n", "3. Use the `inspect` tool to sift through the HTML.\n", "4. Parse HTML with the `BeautifulSoup` library.\n", "5. Store data in a `csv` file using the `Pandas` library.\n", "\n", "# Let's scrape some data!\n", "The data we are scraping today will be from the [Quotes to Scrape](http://quotes.toscrape.com/) website.\n", "\n", "\n", "## Step 1:\n", "> **Import the necessary tools for our project**\n", "\n", "![](https://media.giphy.com/media/KcE7Dq5f8TTXzZ1LAF/giphy.gif)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:29.062725Z", "start_time": "2020-12-17T14:36:28.151653Z"}}, "outputs": [], "source": ["# Webscraping\n", "import requests\n", "from bs4 import BeautifulSoup\n", "\n", "# Data organization\n", "import pandas as pd\n", "\n", "# Visualization\n", "import matplotlib.pyplot as plt\n", "plt.rcParams.update({'font.size': 22})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2\n", "> **We use the `requests` library to connect to the website we wish to scrape.**\n", "\n", "<img src='https://media.giphy.com/media/eCwAEs05phtK/giphy.gif' width='300'></img>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:29.433808Z", "start_time": "2020-12-17T14:36:29.217508Z"}}, "outputs": [], "source": ["url = 'http://quotes.toscrape.com'\n", "response = requests.get(url)\n", "response"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u2705**A `Response 200` means our request was sucessful!** \u2705\n", "\n", "\u274cLet's take a quick look at an *unsuccessful* response. \u274c"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:30.454985Z", "start_time": "2020-12-17T14:36:30.355927Z"}}, "outputs": [], "source": ["bad_url = 'http://quotes.toscrape.com/20'\n", "requests.get(bad_url)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**A `Response 404` means that the url you are using in your request is not pointing to a valid webpage.**\n", "\n", "<img src='https://media.giphy.com/media/VwoJkTfZAUBSU/giphy.gif' width='300'></img>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3\n", "> **We collect the html from the website by adding `.text` to the end of the response variable.** "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:31.709648Z", "start_time": "2020-12-17T14:36:31.706135Z"}}, "outputs": [], "source": ["html = response.text\n", "html[:50]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4\n", "> **We use `BeautifulSoup` to turn the html into something we can manipulate.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:32.709133Z", "start_time": "2020-12-17T14:36:32.676477Z"}}, "outputs": [], "source": ["soup = BeautifulSoup(html, 'lxml')\n", "soup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><h1><b><i><u>Very Soupy</u></i></b></h1></center>\n", "\n", "The name ***Beautiful Soup*** is an appropriate description. HTML does not make for a lovely reading experience. If you feel like you're staring at complete gibberish, you're not entirely wrong! HTML is a language designed for computers, not for human eyes.\n", "\n", "<img src='https://media.giphy.com/media/5xtDarBbqdSQxfGFdNS/giphy.gif' width=\"200\"></img>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Fortunately for us,** <u>we do not have to read through every line of the html in order to web scrape.</u> \n", "\n", "Modern day web browsers come equipped with tools that allow us to easily sift through this soupy text.\n", "\n", "\n", "## Step 4\n", ">**We open up the page we're trying to scrape in a new tab.** <b><a href='http://quotes.toscrape.com/' target='_blank'>Click Here!</a></b> \ud83d\udc40"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5\n", "   > **We create a list of every `div` that has a `class` of \"quote\"**\n", "\n", "**In this instance,** every item we want to collect is divided into identically labeled containers.\n", "- A div with a class of 'quote'.\n", "\n", "Not all HTML is as well organized as this page, but HTML is basically just a bunch of different organizational containers that we use to divide up text and other forms of media. Figuring out the organizational structure of a website is the entire process for web scraping. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:34.815646Z", "start_time": "2020-12-17T14:36:34.811086Z"}}, "outputs": [], "source": ["quote_divs = soup.find_all('div', {'class': 'quote'})\n", "len(quote_divs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 6\n", "> **To figure out how to grab all the datapoints from a quote, we isolate a single quote, and work through the code for a single `div`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:35.784900Z", "start_time": "2020-12-17T14:36:35.782432Z"}}, "outputs": [], "source": ["first_quote = quote_divs[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### First we grab the text of the quote"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:36.691562Z", "start_time": "2020-12-17T14:36:36.687578Z"}}, "outputs": [], "source": ["text = first_quote.find('span', {'class':'text'})\n", "text = text.text\n", "text"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Next we grab the author"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:37.668870Z", "start_time": "2020-12-17T14:36:37.665039Z"}}, "outputs": [], "source": ["author = first_quote.find('small', {'class': 'author'})\n", "author_name = author.text\n", "author_name"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's also grab the link that points to the author's bio!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:38.574856Z", "start_time": "2020-12-17T14:36:38.570959Z"}}, "outputs": [], "source": ["author_link = author.findNextSibling().attrs.get('href')\n", "author_link = url + author_link\n", "author_link"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### And finally, let's grab all of the tags for the quote"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:39.524461Z", "start_time": "2020-12-17T14:36:39.519488Z"}}, "outputs": [], "source": ["tag_container = first_quote.find('div', {'class': 'tags'})\n", "tag_links = tag_container.find_all('a')\n", "\n", "tags = []\n", "for tag in tag_links:\n", "    tags.append(tag.text)\n", "    \n", "tags"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Our data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:40.449096Z", "start_time": "2020-12-17T14:36:40.445170Z"}}, "outputs": [], "source": ["print('text:', text)\n", "print('author name: ', author_name)\n", "print('author link: ', author_link)\n", "print('tags: ', tags)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Step 7\n", "> We create a function to make out code reusable.\n", "\n", "Now that we know how to collect this data from a quote div, we can compile the code into a [function](https://www.geeksforgeeks.org/functions-in-python/) called `quote_data`. This allows us to grab a quote div, feed it into the function like so...\n", "> `quote_data(quote_div)`\n", "\n", "...and receive all of the data from that div."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T15:55:43.597063Z", "start_time": "2020-12-17T15:55:43.594126Z"}}, "outputs": [], "source": ["def quote_data(quote_div):\n", "    pass\n", "    # Collect the quote\n", "    # YOUR CODE HERE\n", "    \n", "    # Collect the author name\n", "    # YOUR CODE HERE\n", "    \n", "    # Collect author link\n", "    # YOUR CODE HERE\n", "    \n", "    # Collect tags\n", "    # YOUR CODE HERE\n", "       \n", "    # Return data as a dictionary\n", "    # YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's test our fuction."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:42.445584Z", "start_time": "2020-12-17T14:36:42.441722Z"}}, "outputs": [], "source": ["quote_data(quote_divs[7])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can collect the data from every quote on the first page with a simple [```for loop```](https://www.w3schools.com/python/python_for_loops.asp)!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:43.464374Z", "start_time": "2020-12-17T14:36:43.457681Z"}, "scrolled": true}, "outputs": [], "source": ["page_one_data = []\n", "for div in quote_divs:\n", "    # Apply our function on each quote\n", "    data_from_div = quote_data(div)\n", "    page_one_data.append(data_from_div)\n", "    \n", "print(len(page_one_data), 'quotes scraped!')\n", "page_one_data[:2]"]}, {"cell_type": "markdown", "metadata": {"ExecuteTime": {"end_time": "2020-07-03T18:55:14.762708Z", "start_time": "2020-07-03T18:55:14.758667Z"}}, "source": ["# We just scraped an entire webpage!\n", "\n", "![](https://media.giphy.com/media/KiXl0vfc9XIIM/giphy.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Level Up: What if we wanted to scrape the quotes from *every* page?\n", "\n", "**Step 1:** The first thing we do is take the code from above that scraped the data for all of the quotes on page one, and move it into a function called `scrape_page`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:45.074990Z", "start_time": "2020-12-17T14:36:45.071872Z"}}, "outputs": [], "source": ["def scrape_page(quote_divs):\n", "    data = []\n", "    for div in quote_divs:\n", "        div_data = quote_data(div)\n", "        data.append(div_data)\n", "        \n", "    return data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 2:** We grab the code we used at the very beginning to collect the html and the list of divs from a web page."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:46.170275Z", "start_time": "2020-12-17T14:36:46.068837Z"}}, "outputs": [], "source": ["base_url = 'http://quotes.toscrape.com'\n", "response = requests.get(url)\n", "html = response.text\n", "soup = BeautifulSoup(html, 'lxml')\n", "quote_divs = soup.find_all('div', {'class': 'quote'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 3:** We feed all of the `quote_divs` into our newly made `parse_page` function to grab all of the data from that page."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:47.080077Z", "start_time": "2020-12-17T14:36:47.076223Z"}}, "outputs": [], "source": ["data = scrape_page(quote_divs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:47.556886Z", "start_time": "2020-12-17T14:36:47.552988Z"}}, "outputs": [], "source": ["data[:2]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 4:** We check to see if there is a `Next Page` button at the bottom of the page.\n", "\n", "*This requires multiple steps.*\n", "\n", "1. We grab the outer container that has a class of `pager`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:48.572878Z", "start_time": "2020-12-17T14:36:48.569609Z"}}, "outputs": [], "source": ["pager = soup.find('ul', {'class': 'pager'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If there is no pager element on the webpage, pager will be set to `None`.\n", "\n", "2. We use an if check to make sure a pager exists:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:49.509072Z", "start_time": "2020-12-17T14:36:49.506395Z"}}, "outputs": [], "source": ["if pager:\n", "    next_page = pager.find('li', {'class': 'next'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. We then check to see if a `Next Page` button exists on the page. \n", "\n", "    - Every page has a next button except the *last* page which only has a `Previous Page` button. Basically, we're checking to see if the `Next button` exists. It it does, we \"click\" it."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:50.514710Z", "start_time": "2020-12-17T14:36:50.511955Z"}}, "outputs": [], "source": ["if next_page:\n", "    next_page = next_page.findChild('a')\\\n", "                         .attrs\\\n", "                         .get('href')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With most webscraping tools, \"clicking a button\" means collecting the link inside the button and making a new request.\n", "\n", "If a link is pointing to a page on the same website, the links are usually just the forward slashes that need to be added to the base website url. This is called a `relative` link.\n", "\n", "**Step 5:** Collect the relative link that points to the next page, and add it to our base_url"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:51.442471Z", "start_time": "2020-12-17T14:36:51.439092Z"}}, "outputs": [], "source": ["next_page = url + next_page\n", "next_page"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Step 6:** We repeat the exact same process for this new link!\n", "\n", "ie:\n", "1. Make request using a url that points to the next page.\n", "2. Scrape quote divs\n", "3. Collect data from every quote div on that page\n", "4. Find the `Next page` button.\n", "5. Collect the url from the button\n", "6. Repeat\n", "\n", "So how do we do this over and over again without repeating ourselves?\n", "\n", "The first step is compile all of these steps into a new function called `scrape_quotes`.\n", "\n", "The second step is, something called `recursion`. \n", "\n", "<center><h1><u>Recursion</u></h1></center>\n", "\n", "![](https://media.giphy.com/media/l1J9R1Q7LJGSZOxFe/giphy.gif)\n", "\n", "> **Recursion** is a bit of a mind bend, so don't feel bad if it is hard to wrap your brain around. It took me a while to be able to understand recursive functions!\n", "\n", "Essentially, recursion is where we use a function inside of itself.\n", "\n", "**In this instance,** our code will be telling the computer, if there is a `Next page` button, rerun all of the code again on the page the next button points us to and check to see if there is a `Next page` button on *that* page. If there is, keep repeating the process until a `Next page` button is not found."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:52.493086Z", "start_time": "2020-12-17T14:36:52.487569Z"}}, "outputs": [], "source": ["def scrape_quotes(url):\n", "    base_url = 'http://quotes.toscrape.com'\n", "    response = requests.get(url)\n", "    html = response.text\n", "    soup = BeautifulSoup(html, 'lxml')\n", "    quote_divs = soup.find_all('div', {'class': 'quote'})\n", "    data = scrape_page(quote_divs)\n", "    \n", "    pager = soup.find('ul', {'class': 'pager'})\n", "    if pager:\n", "        next_page = pager.find('li', {'class': 'next'})\n", "        \n", "        if next_page:\n", "            next_page =  next_page.findChild('a')\\\n", "                                  .attrs\\\n", "                                  .get('href')\n", "            \n", "            next_page = base_url + next_page\n", "            print('Scraping', next_page)\n", "            ## This is where the recursion happens\n", "            data += scrape_quotes(next_page)\n", "   \n", "    return data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can set a variable called `data` that is the output of our recursive function!\n", "\n", "> A print statement has been added to output what page is being scraped"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:54.500679Z", "start_time": "2020-12-17T14:36:53.498624Z"}, "scrolled": true}, "outputs": [], "source": ["data = scrape_quotes(url)\n", "print(len(data), 'Quotes scraped!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can visualize and explore our data!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:36:57.355462Z", "start_time": "2020-12-17T14:36:57.117820Z"}}, "outputs": [], "source": ["def count_tags(quote):\n", "    return len(quote['tags'])\n", "\n", "def tag_lengths(data):\n", "    lengths = []\n", "    for quote in data:\n", "        lengths.append(count_tags(quote))\n", "        \n", "    return lengths\n", "        \n", "lengths = tag_lengths(data)\n", "plt.figure(figsize=(10,6))   \n", "plt.hist(lengths, bins=9)\n", "plt.title('Number of Tags');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Saving our data\n", "\n", "There are multiple ways to save data to a file. Pandas, `The Excel of Python` allows us to do this easily."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:37:00.236772Z", "start_time": "2020-12-17T14:37:00.223278Z"}}, "outputs": [], "source": ["df = pd.DataFrame(data)\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-07-10T17:22:10.108560Z", "start_time": "2020-07-10T17:22:10.097506Z"}}, "outputs": [], "source": ["df.to_csv('quotes_data.csv', index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Scraping Tables with Pandas"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T16:15:30.994878Z", "start_time": "2020-12-17T16:15:30.512000Z"}}, "outputs": [], "source": ["url = 'https://en.wikipedia.org/wiki/List_of_current_United_States_senators'\n", "pd.read_html(url, attrs={'id': 'senators'})[0].head()"]}, {"cell_type": "markdown", "metadata": {"ExecuteTime": {"end_time": "2020-12-17T14:57:27.424147Z", "start_time": "2020-12-17T14:57:26.425891Z"}}, "source": ["Pandas `read_html` is a really handy tool, but it isn't perfect. You'll notice that the resulting table above is quite messy. The urls for the portrait column are replaced 'NaN' values, the Born column is returning the age of the senator instead of their birth date, the occupation and previous office columns are lists inside strings with the spacing between letters removed. For this table, if we wanted to take full advantage of the available data, we would need to scrape this table by hand.  \n", "\n", "To do that, we can begin by isolating the table, and then isolating the data rows in the table.\n", "\n", "Below is a function that returns a cleaned version of the senators table."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T15:52:31.742225Z", "start_time": "2020-12-17T15:52:31.729504Z"}}, "outputs": [], "source": ["def scrape_senate_table():\n", "    # Connect to the wikipedia page\n", "    response = requests.get('https://en.wikipedia.org/wiki/List_of_current_United_States_senators')\n", "    # Collect the html from the page\n", "    html = response.text\n", "    # Parse the html with BeautifulSoup\n", "    soup = BeautifulSoup(html)\n", "    # Find the senators table\n", "    table = soup.find('table', {'id': 'senators'})\n", "    \n", "    # The first row in the table contains the column names.\n", "    # Isolate the first row\n", "    rows = table.find_all('tr')[1:]\n", "    # Create an empty list to append the row data to\n", "    senate_data = []\n", "    # Create some cleaning functions for text data\n", "    remove_new_line = lambda x: x.replace('\\n', '')\n", "    split_new_line = lambda x: x.split('\\n')\n", "    \n", "    # Loop over each row\n", "    for row in rows:\n", "        # find all td tags from the row\n", "        td_tags = row.find_all('td')\n", "        # The senators table merges the cell for `state name` so it spans both \n", "        # senators from a state. \n", "        # When parsing the html, the state name only appears for the senator\n", "        # that appears first in the table, and the second appearing senator has \n", "        # one less td tag.\n", "        # Because of this we need to check the length of the td tags\n", "        # If there is one less tag, we use the state name from the previous iteration\n", "        if len(td_tags) == 10:\n", "            previous_element = [td_tags[0]]\n", "        else:\n", "            previous_element += td_tags\n", "            td_tags = previous_element\n", "        \n", "        # Collect the state name\n", "        state = remove_new_line(td_tags[0].text)\n", "        # Collect the image url\n", "        image = td_tags[1].find('img').attrs['src']\n", "        # Collect the name of the senator\n", "        name = remove_new_line(row.find('th').text)\n", "        # Collect the css color string for the political party\n", "        party_color = td_tags[2].attrs['style'].split(':')[-1]\n", "        # Collect the party name\n", "        party_name = remove_new_line(td_tags[3].text)\n", "        # Collect the date of birth\n", "        dob = ' '.join(remove_new_line(td_tags[4]\\\n", "                                       .text)\\\n", "                                       .strip()\\\n", "                                       .split(' ')[1:4])\n", "        # Collect the occupation\n", "        occupation = split_new_line(BeautifulSoup(str(td_tags[5])\\\n", "                                                  .replace('<br/>', '\\n'))\\\n", "                                                  .td\\\n", "                                                  .text)\n", "        # If only one occupation is present\n", "        # Pull that occupation out of the list\n", "        # And return a single string\n", "        if occupation[1] == '':\n", "            occupation = occupation[0]\n", "        # Collect the previous office\n", "        previous_office = split_new_line(BeautifulSoup(str(td_tags[6])\\\n", "                                                       .replace('<br/>', '\\n'))\\\n", "                                                       .td\\\n", "                                                      .text)\n", "        # If only one previous office is present\n", "        # Pull that value out of the list\n", "        # And return a single string\n", "        if previous_office[1] == '':\n", "            previous_office = previous_office[0]\n", "        # Collect assumed office\n", "        assumed_office = remove_new_line(td_tags[7].text)\n", "        # Collect the end of their term\n", "        term_up = remove_new_line(td_tags[8].text)\n", "        # Collect their residence\n", "        residence = split_new_line(td_tags[9].text)\n", "        \n", "        # If only one residence is present\n", "        # Pull that value out of the list\n", "        # And return a single string\n", "        if residence[1] == '':\n", "            residence = residence[0]\n", "            # Many of the residences have a \n", "            # link pointing to additional information\n", "            # We do not need this\n", "            if '[' in residence:\n", "                residence = residence[:-3]\n", "        # Create data dictionary\n", "        collected = {'state': state, 'name': name,'dob': dob, 'party': party_name,\n", "                    'party_color': party_color,\n", "                    'occupation': occupation,\n", "                    'previous_office': previous_office,\n", "                    'assumed_office': assumed_office,\n", "                    'term_up': term_up, 'residence': residence,\n", "                    'portrait': image}\n", "        # Append dictionary to the senate_data list\n", "        senate_data.append(collected)\n", "        \n", "    # Once data from all rows has been collected\n", "    # return the data as a pandas dataframe\n", "    return pd.DataFrame(senate_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2020-12-17T16:15:46.355808Z", "start_time": "2020-12-17T16:15:45.813067Z"}}, "outputs": [], "source": ["scrape_senate_table().head(3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Web scraping is a powerful tool. \n", "\n", "It can be used:\n", "- To discover the most in demand skills for thousands of online job postings.\n", "- To learn the average price of a product from thousands of online sale.\n", "- To research social media networks.\n", "\n", "**And so much more!** As our world continues to develop online markets and communities, the uses for webscraping continue to grow. In the end, the power of web scraping comes from the ability to create datasets that otherwise do not exist, or at the very least, are not readily available to the public.\n"]}], "metadata": {"hide_input": false, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "303.797px"}, "toc_section_display": true, "toc_window_display": false}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 2}