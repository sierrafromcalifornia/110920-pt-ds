{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Sampling, the Central Limit Theorem, and Confidence Intervals\n", "\n", "![sample](https://media.giphy.com/media/OsOP6zRwxrnji/giphy.gif)\n", "\n", "\n", "# Agenda \n", "\n", "1. Differentiate terms: descriptive/inferential, statistics and parameters, sample distribution/sampling distribution\n", "2. Describe the central limit theorem and connect it to our knowledge of distributions and sampling.\n", "3. Define and calculate standard error\n", "5. Confidence intervals"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probability vs Statistics\n", "- Probability starts with known probabilities and obtains how probable any particular observation would be\n", "- Statistics works the other way around. Start with and observations (data) and try to determine its probability\n", "\n", "[This article](https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/node12.html#:~:text=Probability%20deals%20with%20predicting%20the,the%20consequences%20of%20mathematical%20definitions.) provides a stellar summary of the difference between probability and statistics.\n", ">In summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to to measure the extent to which our world is ideal."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Descriptive vs Inferential Statistics\n", "- Descriptive Statistics\n", "   > simply describe what is observed. The average height of a high school football team can be directly calculated by measuring all of the current players height.\n", "- Inferential statistics \n", "    > try to say something general about a larger group of subjects than those we have measured. For example, we would be doing inferential statistics if we wanted to know about the average height of all high school football teams.\n", "    - To put it another way, statistical inference is the process by which we take observations of a subset of a group and generalize to the whole group."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["In order for us to make any determinations about a population, we must first get information about it.\n", "\n", "Because it's usually completely impractical to get data about *everyone* in a population, we must take a sample.\n", "\n", "## Key Terms\n", " - the entire group is known as the **population**  \n", " - the subset is a known as the **sample**\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![pop](./img/sample_pop.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Population v Sample Terminology\n", "Characteristics of populations are called **parameters**\n", "\n", "Characteristics of a sample are called **statistics**\n", "\n", "A sample statistic is a **point estimate** of the population parameter\n", "\n", "![imgsample](./img/sample_stats.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Word Exercise \n", "Put the variables in the correct place.\n"]}, {"cell_type": "code", "execution_count": 29, "metadata": {"ExecuteTime": {"end_time": "2021-01-19T20:44:59.464252Z", "start_time": "2021-01-19T20:44:59.459014Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["We sampled 40 bee hives and calcuted the mean colony population \n", "          to be 75,690 bees. 75,690 is a point estimate of the population paramter\n", "\n", "We repeatedly sample 40 people at random from Seattle and \n", "        measure their heart rate,then calculate the mean of each sample. \n", "        We call the plot of this collection of statistics\n", "        the sampling distribution.\n", "        \n", "There are exactly 58 Javan Rhino's left in the wild. \n", "        Their mean length has been measured accurately at 5 feet.\n", "        This mean length is considered a population parameter. \n", "        \n", "If we plot a histogram of individual pistil lengths \n", "      measured on 50 hibiscus flowers, we would be plotting the distribution \n", "      of an attribute of our sample of hibiscus flowers. \n", "        \n", "Since every restaurant in Chicago is required by law to register\n", "        with the city, we can accurately count the number of active pizza restaurants\n", "        actively operating right now.  This group represents the population of actively \n", "        operating, registered pizza restaurants in Chicago.\n", "    \n", "The mean number of hourly hits to Jelle's Marble Racing website \n", "            randomly sampled across a seven day period represents a sample\n", "            statistic.\n", "        \n"]}], "source": ["# Word Exercise\n", "\n", "var_1 = 'population'\n", "var_2 = 'sample'\n", "var_3 = 'point estimate'\n", "var_4 = 'statistic'\n", "var_5 = 'parameter'\n", "var_6 = 'sampling'\n", "\n", "\n", "print(f\"\"\"We sampled 40 bee hives and calcuted the mean colony population \n", "          to be 75,690 bees. 75,690 is a {var_3} of the population paramter\\n\"\"\")\n", "\n", "print(f\"\"\"We repeatedly sample 40 people at random from Seattle and \n", "        measure their heart rate,then calculate the mean of each sample. \n", "        We call the plot of this collection of statistics\n", "        the {var_6} distribution.\n", "        \"\"\")\n", "\n", "print(f\"\"\"There are exactly 58 Javan Rhino's left in the wild. \n", "        Their mean length has been measured accurately at 5 feet.\n", "        This mean length is considered a population {var_5}. \n", "        \"\"\")\n", "\n", "print(f\"\"\"If we plot a histogram of individual pistil lengths \n", "      measured on 50 hibiscus flowers, we would be plotting the distribution \n", "      of an attribute of our {var_2} of hibiscus flowers. \n", "        \"\"\")\n", "\n", "print(f\"\"\"Since every restaurant in Chicago is required by law to register\n", "        with the city, we can accurately count the number of active pizza restaurants\n", "        actively operating right now.  This group represents the {var_1} of actively \n", "        operating, registered pizza restaurants in Chicago.\n", "    \"\"\")\n", "\n", "print(f\"\"\"The mean number of hourly hits to Jelle's Marble Racing website \n", "            randomly sampled across a seven day period represents a sample\n", "            {var_4}.\n", "        \"\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A Simulation to Reinforce Our Definitions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's create a population of systolic blood pressure of adult males in Chicago, assuming a mean of 114 mmHg with a standard deviation of 11 mmHg.  We will also assume the adult male population to be 1.5 million. \n", "\n", "It is impossible to measure the systolic blood pressure of every man in Chicago, but let's assume multiple investigations have led to the conclusion the the mean and std of this population is 114 and 11, respectively. These are therefore estimators of the population parameter.\n", "\n", "$\\Large\\hat\\mu = 114$  \n", "$\\Large\\hat\\sigma = 11$\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "pop = int(1.5*10**6)\n", "# Use numpy to generate a normal distribution of the \n", "sys_pop = np.random.normal(loc=114, scale=11, size=pop)\n", "\n", "fig, ax = plt.subplots()\n", "\n", "sns.kdeplot(sys_pop, ax=ax, shade=True)\n", "ax.set_title('Distribution of Adult Male Systolic Blood Pressure')\n", "ax.set_xlabel('Systolic BP');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's then imagine we develop an effective manner of random sampling, and simulate with numpy. Our sample size is 40 people.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sample_size = 40\n", "sample = np.random.choice(sys_pop, sample_size)\n", "\n", "# We can look at the distribution of the values in the sample."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "sns.distplot(sample, ax=ax, bins=15)\n", "ax.set_title('Sample Distribution of Systolic BP Measurements');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can then calculate the sample statistics:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f'Sample mean: {sample.mean()}')\n", "print(f'Sample standard deviation: {sample.std()}')\n", "print(f'Sample median: {np.median(sample)}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we repeated this process, taking samples of the population repeatedly, we would get an array of sample statistics."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["number_of_samples = 1000\n", "sample_size = 40\n", "sample_stats = []\n", "\n", "for _ in range(number_of_samples):\n", "    sample = np.random.choice(sys_pop, sample_size)\n", "    # collect the mean of each of the 1000 samples in sample stats\n", "    sample_stats.append(sample.mean())\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The collection of sample stats represents our __sampling distribution__"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots()\n", "ax.hist(sorted(sample_stats), bins=20)\n", "ax.set_title('Sampling Distribution\\n of Systolic BP')\n", "ax.set_xlabel(\"Systolic Blood Pressure\")\n", "ax.set_ylabel('Count');"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# 3. Central Limit Theorem"]}, {"cell_type": "markdown", "metadata": {}, "source": ["An interesting property of this sampling distribution:\n", "    \n", "As we continue to sample, the mean of the sampling distribution gets closer and closer to the population mean."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If we take repeated samples of a population, the sampling distribution of sample means will approximate to a normal distribution, no matter the underlying distribution!\n", "\n", "[good D3 example](https://seeing-theory.brown.edu/probability-distributions/index.html)\n", "\n", "[good video demonstration](https://www.youtube.com/watch?v=jvoxEYmQHNM)\n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's look at an example taken from the ubiquitous Iris dataset. This histogram represents the distributions of sepal length:\n", "\n", "\n", "![probgif](./img/probability-basics.gif)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["exponential = np.random.exponential(size = 10000)\n", "number_of_samples = 1000\n", "sample_size = 40\n", "sample_stats = []\n", "\n", "for _ in range(number_of_samples):\n", "    sample = np.random.choice(exponential, sample_size)\n", "    # collect the mean of each of the 1000 samples in sample stats\n", "    sample_stats.append(sample.mean())\n", "    \n", "fig, ax = plt.subplots(1,2)\n", "ax[0].hist(exponential)\n", "ax[0].set_title('Original Distribution')\n", "ax[1].hist(sorted(sample_stats), bins=20)\n", "ax[1].set_title('Sampling Distribution\\n of Exponential');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["https://www.kaggle.com/tentotheminus9/central-limit-theorem-animation\n", "\n", "As we will see in hypothesis testing, pairing this theorem with the Empirical rule will be very powerful.\n", "\n", "![empirical](img/empirical_rule.png)\n", "\n", "\n", "Knowing that any sampling distribtion, no matter the underlying population distribution, will approach normality, we will be able to judge, given the empirical rule, how rare a given sample statistic is. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Confidence Intervals"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Standard Error of the Mean"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The standard error of the mean is the standard deviation of the sampling distribution.\n", "The issue is that a sample is not an exact replica of the population. We need to account for that fact in order to make our estimate of the $\\mu$ value possible. Let's break it down:\n", "\n", "**Population sigma** <br/>\n", "\n", "$\\large\\sigma _{x} = \\frac{\\sigma }{\\sqrt{n}}$\n", "\n", "* $ \\sigma _{x}$ = standard error of $\\bar{x} $\n", "* $ \\sigma $ = standard deviation of population"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**What if we do not know the population sigma?**<br>\n", "If we do not know the population standard deviation, we can approximate it by using the sample standard deviation.\n", "\n", "$\\large\\sigma _{x} \u2248 \\frac{s}{\\sqrt{n}}$\n", "\n", "* s = sample standard deviation"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**Sample size impact on standard error of mean**<br>\n", "\n", "How should sample size influence standard error of the mean?\n", "\n", "It will get *smaller* as sample size *increases*\n", "\n", "![error](./img/diminishing_error.png)  \n", "Important implication: The Standard Error of the mean remains the same as long as the population standard deviation is known and sample size remains the same.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def standard_error(distribution, largest_sample_size, population_std=None):\n", "    \n", "    '''\n", "    Calculate the standard errors for a range of sample sizes\n", "    to demonstrate how standard error decreases when sample \n", "    size increases.\n", "    '''\n", " \n", "    std_errors = {}\n", "    \n", "    for sample_size in range(50,largest_sample_size+1):\n", "        sample = np.random.choice(distribution, size=sample_size, replace=True)\n", "        # Standard error with sample distribution standard deviation \n", "        # in place of population\n", "        if population_std == None:\n", "            std_err = np.std(sample)/np.sqrt(sample_size)\n", "            std_errors[sample_size] = std_err\n", "        \n", "        else:\n", "            std_err = population_std/np.sqrt(sample_size)\n", "            std_errors[sample_size] = std_err\n", "        \n", "    return std_errors\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["std_errors = standard_error(sys_pop, 1000)\n", "\n", "fig, ax = plt.subplots()\n", "\n", "sns.scatterplot(list(std_errors.keys()), list(std_errors.values()))\n", "ax.set_ylabel('Standard Error')\n", "ax.set_xlabel('Sample Size')\n", "ax.set_title('Standard Error vs. Sample Size -- Sample');"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["std_errors = standard_error(sys_pop, 1000, population_std=11)\n", "\n", "fig, ax = plt.subplots()\n", "\n", "sns.scatterplot(list(std_errors.keys()), list(std_errors.values()))\n", "ax.set_ylabel('Standard Error')\n", "ax.set_xlabel('Sample Size')\n", "ax.set_title('Standard Error vs. Sample Size -- Population');"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from scipy import stats\n", "from matplotlib import pyplot as plt\n", "\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Because sample statistics are imperfect representations of the true population values, it is often appropriate to state these estimates with **confidence intervals**.\n", "\n", "Suppose I weigh a sample of 50 jellybeans from a population of 10000 and find the average weight to be 1.25 grams. Can I take this figure to be a good estimate of the average weight over the whole *population* of jelly beans?\n", "\n", "In a word, yes. (What else do I have to go on!?) But what I want now is a more or less precise way of indicating that this figure, though likely close to the real population mean, is inexact.\n", "\n", "Natural idea: I'll say I'm confident that the real population value lies in some neighborhood or *interval* around the figure I obtained for my sample.\n", "\n", "Notice that:\n", "\n", "- The larger my sample, the more confident I may be about the sample's representativeness for the whole population;\n", "- The larger I make the interval, the more confident I may be about the true population value falling within it.\n", "\n", "## Interpretation\n", "\n", "Here's another example: Suppose our Indian correspondent (or David Attenborough) takes several hundred measurements of parrot beak lengths in the Ganges river basin and calculates (correctly!) an average beak length of 9cm. He reports this measure by saying that the 90%-confidence interval is (8.6, 9.4).\n", "\n", "This does NOT mean that the true population mean beak length has a 90% chance of being somewhere between 8.6cm and 9.4cm. After all, the true mean either falls in that range or it doesn't. The notion of probability *here* doesn't seem to make much sense. Rather, what our correspondent means is that, if we were to conduct the same measuring experiment many times, constructing intervals in the same way, we should expect 90% of those intervals to contain the true population mean.\n", "\n", "## Construction\n", "\n", "OK: So how do we construct these intervals?\n", "\n", "The confidence interval we construct will depend on the statistics of our sample. It will depend in particular on (i) our sample mean and (ii) our sample size.\n", "\n", "It will also depend on the underlying distribution of our data. If our data are **normally** distributed, then we can proceed as follows:\n", "\n", "Naturally, the confidence interval will be centered on our sample mean. To construct the endpoints we step out from the center with a step size equal to the standard error, $\\large\\frac{\\sigma}{\\sqrt{n}}$. The number of steps we take is determined by which level of confidence we want attached to our interval: In particular, we take $z$-many steps, where $z$ is the (two-tailed) z-score that corresponds to our chosen level of confidence.\n", "\n", "If our data are **not** normally distributed, then there are several strategies we might try, some of which ultimately depend on some connection to the normal distribution, like a strategy that appeals to the Central Limit Theorem.\n", "\n", "## CIs for Normally Distributed Data\n", "\n", "Let's look at an example with data we assume to be normally distributed:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# A normally distributed variable with a\n", "# population size of 1000\n", "\n", "pop = list(stats.norm.rvs(size=1000,\n", "                          random_state=42))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's calculate the population mean.\n", "\n", "np.mean(pop)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# And the population standard deviation.\n", "\n", "pop_std = np.std(pop)\n", "pop_std"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's say we take a sample of fifty from our population, and that we want an 80%-confidence interval for our estimate of the population mean. The z-score that corresponds to an 80%-confidence interval is:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z = stats.norm.ppf(0.9)\n", "#Why do we want 0.9 here?\n", "z"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(42)\n", "sample = np.random.choice(a=pop, size=50)\n", "np.mean(sample)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pop_std / np.sqrt(50) * z"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Thus we'd report our estimate of the population mean as 0.177 $\\pm$ 0.177, or, equivalently, as (0, 0.354). Note that the true population mean of 0.0193 is in fact in this range."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CIs for Non-Normally Distributed Data\n", "\n", "One of the most commonly used strategies for dealing with non-normally distributed data is to find a way to reduce the problem to one that involves normally distributed data!\n", "\n", "[Here](https://file.scirp.org/Html/3-1240887_76758.htm) is a review article that compares several different strategies. (Note that it ultimately recommends a sort of Bayesian method. We'll get to Bayesian reasoning in a later lesson.)\n", "\n", "## T-Distribution\n", "\n", "![imgguiness](./img/guiness.png)\n", "\n", "We can use the normal distribution when either:\n", "* the population standard deviation is known\n", "* the sample size is greater than 30.\n", "\n", "If **neither** of these holds true, we need to use the **T-distribution**. The t-distribution is wider and has different critical values for different sample sizes.\n", "\n", "\n", "PDF of T-distribution: ${\\frac {\\Gamma \\left({\\frac {\\nu +1}{2}}\\right)}{{\\sqrt {\\nu \\pi }}\\,\\Gamma \\left({\\frac {\\nu }{2}}\\right)}}\\left(1+{\\frac {x^{2}}{\\nu }}\\right)^{-{\\frac {\\nu +1}{2}}}\\!$, where $\\Gamma$ denotes the [Gamma Function](https://en.wikipedia.org/wiki/Gamma_function).\n", "\n", "parameter: $\\nu > 0$ where $\\nu$ is degrees of freedom (n-1)\n", "\n", "**T distribution becomes closer to Z distribution as n increases**\n", "![zvt](./img/z_vs_t.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from math import gamma\n", "\n", "fig, ax = plt.subplots(7, figsize=(10, 20))\n", "\n", "X = np.linspace(-10, 10, 201)\n", "nus = np.arange(2, 9)\n", "y_norm = 1 / np.sqrt(2*np.pi) * np.exp(-0.5 * X**2)\n", "\n", "for j in range(7):\n", "    y = gamma((nus[j]+1) / 2) / (np.sqrt(np.pi*nus[j]) * gamma(nus[j] / 2)) *\\\n", "(1 + X**2/nus[j])**((-nus[j]+1) / 2)\n", "    ax[j].plot(X, y, label=fr't-Distribution, $\\nu$ = {nus[j]}')\n", "    ax[j].plot(X, y_norm, label='Normal Distribution')\n", "    ax[j].legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CIs for T-Distribution\n", "\n", "The construction of confidence intervals for the t-Distribution is similar to how they are made for the normal distribution. But instead of z-scores, we'll have t-scores. And since we don't have access to the population standard deviation, we'll make use of the sample standard deviation instead.\n", "\n", "left endpt.: $\\bar{x} - t\\times\\frac{s}{\\sqrt{n}}$ <br/>\n", "right endpt.: $\\bar{x} + t\\times\\frac{s}{\\sqrt{n}}$\n", "\n", "### T-Distribution Example\n", "\n", "You are inspecting a hardware factory and want to construct a 90% confidence interval of acceptable screw lengths. You draw a sample of 30 screws and calculate their mean length as 4.8 centimeters and the standard deviation as 0.4 centimeters. What are the bounds of your confidence interval?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy import stats"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stats.t.ppf(0.95, n-1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n = 30\n", "mean = 4.8\n", "t_value = stats.t.ppf(0.95, n-1)\n", "margin_error = t_value * 0.4/(n**0.5)\n", "confidence_interval = (mean - margin_error, mean + margin_error)\n", "\n", "confidence_interval"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your turn!\n", "\n", "# You're weighing walruses in the Arctic in the attempt to estimate\n", "# the mean weight of the Canadian walrus population. You have a sample\n", "# of 30 walrus weights. The mean of the sample is 2000 lbs. and the\n", "# standard deviation is 200 lbs. Calculate the 80%-confidence interval.\n", "# Calculate the 70%-confidence interval. How do they compare to the\n", "# normal-distribution CIs? (To calculate the latter, just use the \n", "# sample standard deviation.)\n", "n = 30\n", "x_bar = 2000\n", "s = 200\n", "\n", "t_value80 = stats.t.ppf(0.9, n-1)\n", "t_value70 = stats.t.ppf(0.85, n-1)\n", "\n", "margin_error80 = t_value80 * 200/(n**0.5)\n", "margin_error70 = t_value70 * 200/(n**0.5)\n", "\n", "conf_int80 = (x_bar - margin_error80, x_bar + margin_error80)\n", "conf_int70 = (x_bar - margin_error70, x_bar + margin_error70)\n", "\n", "z_score80 = stats.norm.ppf(0.9)\n", "z_score70 = stats.norm.ppf(0.85)\n", "\n", "error_norm80 = z_score80 * 200/(n**0.5)\n", "error_norm70 = z_score70 * 200/(n**0.5)\n", "\n", "conf_norm80 = (x_bar - error_norm80, x_bar + error_norm80)\n", "conf_norm70 = (x_bar - error_norm70, x_bar + error_norm70)\n", "\n", "print(conf_int80)\n", "print(conf_int70)\n", "print(conf_norm80)\n", "print(conf_norm70)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Binomial Distribution\n", "\n", "What if we have a binomial distribution? Suppose we have the following sample statistic:\n", "\n", "A survey of 3000 voters found that 1245 approved of the job the governor was doing. How can we express our 95%-confidence level about voter approval of the governor among _all_ voters?\n", "\n", "To solve this, we'll once again start with our sample proportion as the center of our CI and step out from it by an amount proportional to the relevant z-score.\n", "\n", "But by how much exactly? Forgoing the [proof](https://newonlinecourses.science.psu.edu/stat414/node/208/), the answer *for suitably large samples* (we're relying here on the Central Limit Theorem) is as follows:\n", "\n", "left endpt.: $\\hat{p} - z\\times\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}$ <br/>\n", "right endpt.: $\\hat{p} + z\\times\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}$\n", "\n", "Let's answer our original question about the voters:\n", "\n", "We have:\n", "\n", "- $\\hat{p} = \\frac{1245}{3000} = 0.415$;\n", "- $n = 3000$;\n", "- $z = 1.96$.\n", "\n", "Therefore:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["p_hat = 0.415\n", "n = 3000\n", "z_voters = stats.norm.ppf(0.975)\n", "step = z_voters * np.sqrt(p_hat * (1-p_hat) / n)\n", "\n", "interval = (p_hat - step, p_hat + step)\n", "interval"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## CIs for Other Parameters\n", "\n", "We might be interested in constructing confidence intervals for other parameters, such as the variance. [This online course](https://newonlinecourses.science.psu.edu/stat414/) has several good examples.\n", "\n", "## A Visual Interpretation of Confidence Intervals\n", "\n", "Let's see if we can get an idea of how confidence intervals work by constructing a plot:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy import stats\n", "def ci_plotter(pop, sample_size, num_samples, conf):\n", "    intervals = []\n", "    for _ in range(num_samples): # number of intervals\n", "        sample = np.random.choice(pop, sample_size)\n", "        step = np.std(pop) / np.sqrt(sample_size) # standard error\n", "        semi_int_size = step * stats.norm.ppf(1 - (1-conf)/2)\n", "        \n", "        x_min = np.mean(sample) - semi_int_size # left endpt. of interval\n", "        x_max = np.mean(sample) + semi_int_size # right endpt. of interval\n", "        intervals.append(np.linspace(x_min, x_max, 30)) # add interval\n", "                                                        # to intervals\n", "    fig, ax = plt.subplots(figsize=(10, 10))\n", "    ax.plot(intervals, range(num_samples), '.') # plot intervals evenly and\n", "                                        # vertically\n", "    ax.vlines(np.mean(pop), 0, num_samples, lw=3);"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 4}