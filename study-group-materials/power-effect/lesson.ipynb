{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Effect Size and Power"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:40:14.073515Z", "start_time": "2021-01-26T14:40:12.732971Z"}}, "outputs": [], "source": ["import numpy as np \n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "from scipy import stats\n", "from statsmodels.stats.power import TTestIndPower\n", "import seaborn as sns\n", "\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Learning goals\n", "After today, you'll be able to ...\n", "- Visualize and explain effect size in python\n", "- Conduct power analysis in python\n", "- Explain what power and power analysis are in context  \n", "- Explain type I and type II error in specific context"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 1. Effect Size\n", "\n", "Effect size is used to quantify the *size* of the difference between two groups under observation. Effect sizes are easy to calculate, understand and apply to any measured outcome and are applicable to a multitude of study domains.\n", "\n", "We know already that $p$-values tell us (together with some $\\alpha$-threshold) *whether* differences between groups are significant; the idea now is to have a measure of *how* significant differences are.\n", "\n", "### Why do we need $p$-values *and* effect size?\n", "\n", "[Here](https://www.physport.org/recommendations/Entry.cfm?ID=93385) is a helpful resource. The basic idea is this:\n", "\n", "We can shrink $p$-values rather artificially by increasing our sample size, no matter how small the effect size between two groups. So, with a large enough sample size, we can have as small a $p$-value as we like\u2013\u2013but it won't be an interesting result if the effect size is small.\n", "\n", "The opposite difficulty is possible as well: Our $p$-value can be quite large if our sample size is small enough. And that's true even if the effect size is large."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Where does gender have a larger effect?\n", "\n", "<img src=\"img/pugs.png\" style=\"width:500px;\">"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Example\n", "Compare effect size of gender in height."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:40:16.010195Z", "start_time": "2021-01-26T14:40:16.004587Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Seed the random number generator so we all get the same results\n", "np.random.seed(10)\n", "\n", "# Mean height and sd for males\n", "male_mean = 178\n", "male_sd = 7.7\n", "\n", "# Generate a normal distribution for male heights \n", "male_height = stats.norm(male_mean, male_sd)\n", "\n", "female_mean = 163\n", "female_sd = 7.3\n", "female_height = stats.norm(female_mean, female_sd)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Collect a random sample"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:42:00.418726Z", "start_time": "2021-01-26T14:42:00.414902Z"}}, "outputs": [], "source": ["male_height.rvs()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:43:07.396262Z", "start_time": "2021-01-26T14:43:07.391976Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def evaluate_PDF(rv, x=4):\n", "    \"\"\"\n", "    Input: a random variable object, number of standard deviations\n", "    Output: x and y values for the normal distribution\n", "    \"\"\"\n", "    \n", "    # Identify the mean and standard deviation of random variable.\n", "    mean = rv.mean()\n", "    std = rv.std()\n", "\n", "    # Use numpy to calculate evenly spaced numbers over the\n", "    # specified interval (4 sd by default) and generate 100 samples.\n", "    xs = np.linspace(mean - x*std, mean + x*std, 100)\n", "    \n", "    # Calculate the normal distribution i.e. the probability density. \n", "    ys = rv.pdf(xs)\n", "\n", "    return xs, ys # Return calculated values"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:45:32.279308Z", "start_time": "2021-01-26T14:45:32.059857Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Male height\n", "xs, ys = evaluate_PDF(male_height)\n", "fig, ax = plt.subplots()\n", "ax.plot(xs, ys, label='male', linewidth=4, color='#beaed4') \n", "\n", "#Female height \n", "xs, ys = evaluate_PDF(female_height)\n", "ax.plot(xs, ys, label='female', linewidth=4, color='#fdc086')\n", "\n", "ax.set_xlabel('height (cm)')\n", "ax.set_ylabel('probability density')\n", "plt.legend();"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Cohen's $d$, standardized metrics for effect size\n", "Cohen\u2019s $d$ is one of the most common ways to measure effect size. As an effect size, Cohen's $d$ is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n", "\n", "$d$ = difference of means / pooled standard deviation;\n", "\n", "![](img/Cohens-d.jpg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T18:56:56.598130Z", "start_time": "2021-01-26T18:56:56.595417Z"}}, "outputs": [], "source": ["def cohen_d(group1, group2):\n", "\n", "    \"\"\"\n", "    Computes Cohen's d.\n", "    Group 1 = pandas series or numpy array\n", "    Group 2 = pandas series of numpy array\n", "    \"\"\"\n", "    \n", "    # Calculate the difference between both group means\n", "\n", "    # Calculate the sample size of both groups\n", "\n", "    # Calculate the sample variance for both groups\n", "\n", "    # Calculate the pooled variance\n", "\n", "    # Calculate Cohen's d statistic\n", "\n", "    # return the d statistic\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T20:38:30.997374Z", "start_time": "2021-01-26T20:38:30.993023Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["np.random.seed(10)\n", "\n", "female_sample = female_height.rvs(1000)\n", "male_sample = male_height.rvs(1000)\n", "\n", "effect = cohen_d(male_sample, female_sample)\n", "print(effect)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Evaluating Effect Size\n", "\n", "[good demo here](https://rpsychologist.com/d3/cohend/)\n", "\n", "Small effect = 0.2\n", "\n", "Medium Effect = 0.5\n", "\n", "Large Effect = 0.8"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:58:59.241330Z", "start_time": "2021-01-26T14:58:59.236553Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def plot_pdfs(cohen_d=2):\n", "    \"\"\"\n", "    Plot PDFs for distributions that differ by some number of stds.\n", "    cohen_d: number of standard deviations between the means\n", "    \"\"\"\n", "    group1 = stats.norm(0, 1)\n", "    group2 = stats.norm(cohen_d, 1)\n", "    xs, ys = evaluate_PDF(group1)\n", "    fig, ax = plt.subplots()\n", "    ax.fill_between(xs, ys, label='Female', color='#ff2289', alpha=0.7)\n", "\n", "    xs, ys = evaluate_PDF(group2)\n", "    ax.fill_between(xs, ys, label='Male', color='#376cb0', alpha=0.7)\n", "    plt.legend()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T14:59:21.933540Z", "start_time": "2021-01-26T14:59:21.725113Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["plot_pdfs()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 2. Error\n", "When conducting hypothesis testing, we __choose__ a value for alpha, which represents the risk of falsely rejecting the null hypothesis. If, as is somewhat conventional, we set the alpha at 0.05, then we are saying that \"for 5% of the time, we are willing to reject the null hypothesis when it is in fact true\". How, then, do we categorize different types of error associated with conducting the experiments?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Type I error\n", "Type I error is usually represented as $\\alpha$, which is the probability of rejecting the null hypothesis when it is in fact true. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Type II Error\n", "Type II error is represented as $\\beta$; it is the probability of failing to reject the null hypothesis when it is in fact false."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 3. Power\n", "How does statistical power relate to two types of error? Power is defined as the __probability of not making a Type\n", "II error__ (i.e., probability of correctly rejecting H0 when it is in fact false)."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["__<center>Power visualized </center>__"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "fragment"}}, "source": ["<img src=\"img/power_1.png\" style=\"width:500px;\">"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["<img src=\"img/power_2.png\" style=\"width:500px;\">"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["<img src=\"img/power_3.png\" style=\"width:500px;\">"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["<img src=\"img/power_4.png\" style=\"width:500px;\">"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Relevance of Power?\n", "- Low Statistical Power: Large risk of committing Type II errors, i.e. a false negative.\n", "- High Statistical Power: Small risk of committing Type II errors."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Elements that affect power:\n", "- Effect Size\n", "- Sample Size (and thus Standard Error)\n", "- Alpha"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["how does sample size affect power?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These four quantities\u2013\u2013$\\alpha$, sample size, effect size, and power\u2013\u2013have important interrelationships for hypothesis testing that this notebook will illustrate.\n", "\n", "Let's recall what each of these is and why each is important to experimental design:\n", "\n", "- $\\alpha$ is our false-positive rate, i.e the rate at which our tests will lead us to reject the null hypothesis when in fact it is true. This is the same $\\alpha$ that determines the size of our test statistic (which in turn affects the size of our confidence intervals). This is semi-standardly set to 0.05, but in practice an appropriate value will depend on the nature of the tests. How costly would a false positive be?\n", "\n", "- Sample Size, often indicated with $n$, is of course just how many points we have in our sample. Note that we may or may not have much control over this! One common scenario is to calculate how large your sample size needs to be in order to have a test that has a given degree of power.\n", "\n", "- Effect Size, commonly measured by Cohen's $d$ statistic, is a parameter over which one has effectively **no control**. This is a measure of *how different* two samples are, and so this is of course a reflection of underlying reality, as opposed to a result of experimental choice.\n", "\n", "- Power is $1 - \\beta$, where $\\beta$ is our false-negative rate. Since $\\beta$ tells us the rate at which our tests will lead us *to fail* to reject the null hypothesis when in fact it is false, the power of a test thus tells us the rate at which our tests lead us to reject the null hypothesis when it is false. Of course, the null hypothesis is false when we have some genuine difference between two samples, and so power is a measure of the ability of a test to pick up on those differences when they are present."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:07:50.359345Z", "start_time": "2021-01-26T15:07:48.913521Z"}}, "outputs": [], "source": ["alpha_default = 0.05\n", "n_default = 100\n", "d_default = 0.5\n", "pow_default = 0.8\n", "\n", "test = TTestIndPower()\n", "\n", "## Power as a function of effect size\n", "ds = np.linspace(0, 1, 100)\n", "power_per_d = []\n", "for d in ds:\n", "    power_per_d.append(test.solve_power(alpha=alpha_default,\n", "                                        nobs1=n_default, effect_size=d))\n", "\n", "## Power as a function of sample size\n", "ns = np.arange(1, 100, 1)\n", "power_per_n = []\n", "for n in ns:\n", "    power_per_n.append(test.solve_power(alpha=alpha_default,\n", "                                        nobs1=n, effect_size=d_default))\n", "\n", "## Power as a function of alpha\n", "alphas = np.linspace(0.01, 0.1, 100)\n", "power_per_alpha = []\n", "for alpha in alphas:\n", "    power_per_alpha.append(test.solve_power(alpha=alpha,\n", "                                            nobs1=n_default, effect_size=d_default))\n", "\n", "\n", "## Sample size as a function of alpha\n", "n_per_alpha = []\n", "for alpha in alphas:\n", "    n_per_alpha.append(test.solve_power(alpha=alpha,\n", "                                        effect_size=d_default, power=pow_default))\n", "\n", "## Plotting\n", "fig, ax = plt.subplots(4, 1, figsize=(10, 20))\n", "ax[0].plot(ds, power_per_d)\n", "ax[0].set_xlabel('Cohen\\'s d')\n", "ax[0].set_ylabel('Power')\n", "ax[0].set_title('The greater the effect size, the more powerful my test!')\n", "ax[1].plot(ns, power_per_n)\n", "ax[1].set_xlabel('Sample Size')\n", "ax[1].set_ylabel('Power')\n", "ax[1].set_title('The larger the sample size, the more powerful my test!')\n", "ax[2].plot(alphas, power_per_alpha)\n", "ax[2].set_xlabel('alpha')\n", "ax[2].set_ylabel('Power')\n", "ax[2].set_title('The larger the false positive rate, the more powerful my test!')\n", "ax[3].plot(alphas, n_per_alpha)\n", "ax[3].set_xlabel('alpha')\n", "ax[3].set_ylabel('Sample Size')\n", "ax[3].set_title('The larger the false positive rate, the smaller the sample\\\n", " I need for a test of a given power!')\n", "plt.tight_layout();"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Part 4. Case Study \n", "Suppose you are launching a pilot study with Instagram and you want to examine whether the new feature\u2013\u2013making the heart when you \"like\" someone's photo red instead of white\u2013\u2013that was developed by the front-end engineer attracted more likes given that other variables are being held constant. You have collected two datasets. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:17:06.050924Z", "start_time": "2021-01-26T15:17:06.038274Z"}}, "outputs": [], "source": ["experiment = pd.read_csv('data/experiment.csv', index_col=0)\n", "control = pd.read_csv('data/control.csv', index_col=0)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### Look at the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:17:06.751590Z", "start_time": "2021-01-26T15:17:06.742161Z"}}, "outputs": [], "source": ["experiment.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:17:45.058621Z", "start_time": "2021-01-26T15:17:45.051970Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["control.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:18:17.633019Z", "start_time": "2021-01-26T15:18:17.627310Z"}}, "outputs": [], "source": ["experiment.mean()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:18:18.061212Z", "start_time": "2021-01-26T15:18:18.056716Z"}}, "outputs": [], "source": ["control.mean()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Calculate effect size:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:18:34.708205Z", "start_time": "2021-01-26T15:18:34.703707Z"}}, "outputs": [], "source": ["effect_size = cohen_d(experiment['Likes_Given_Exp'], control['Likes_Given_Con'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:18:35.555992Z", "start_time": "2021-01-26T15:18:35.552110Z"}}, "outputs": [], "source": ["effect_size"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["#### Solve for sample size\n", "\n", "We can use `power_analysis.solve_power` from `statsmodels` to find the sample size you need.\n", "\n", "[documentation here](https://www.statsmodels.org/dev/generated/statsmodels.stats.power.tt_ind_solve_power.html)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:18:47.156041Z", "start_time": "2021-01-26T15:18:47.143355Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# We want to know how many observations we need in order to\n", "# attain a power of .8, given an alpha of .05\n", " \n", "#effect_size = 0.8\n", "alpha = 0.05 # significance level\n", "power = 0.8\n", "\n", "power_analysis = TTestIndPower()\n", "sample_size = power_analysis.solve_power(effect_size=effect_size, \n", "                                         power=power, \n", "                                         alpha=alpha)\n", "sample_size"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:18:50.798773Z", "start_time": "2021-01-26T15:18:50.795715Z"}, "slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["print(experiment.shape)\n", "print(control.shape)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Luckily, we do have enough observations to conduct this experiment!\n", "\n", "In determining what test we should use, let's compare the variance of each sample. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T15:19:53.445981Z", "start_time": "2021-01-26T15:19:53.442660Z"}}, "outputs": [], "source": ["print(experiment.Likes_Given_Exp.var())\n", "print(control.Likes_Given_Con.var())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The variance of these samples seems notably different. Because of that **the ideal test for these samples is welch's ttest**, which assumes non equal variance between samples. \n", "\n", "As an exercise, let's code out student's and welch's ttests and compare the outputs for both."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Ttest Equal Variance**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](img/ttest_two_samp_equal_var.png)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T17:46:19.400625Z", "start_time": "2021-01-26T17:46:19.393317Z"}}, "outputs": [], "source": ["def pooled_var(a, b):\n", "    # Instantiate a count variable\n", "    # that represents the numerator\n", "    # of the pooled variance\n", "\n", "    # Loop over the values in sample a\n", "\n", "        # Subtract the mean of sample a\n", "        # from the value\n", "\n", "        # Square the difference\n", "\n", "        # Add the squared difference to the \n", "        # count variable\n", "\n", "    # Loop over the values in sample b\n", "\n", "        # Subtract the mean of sample b\n", "        # from the value\n", "\n", "        # Square the difference\n", "\n", "        # Add the squared difference to the \n", "        # count variable\n", "\n", "    \n", "    # Calculate the denominator\n", "    # by adding the length of both samples\n", "    # and subtracting two\n", "\n", "    \n", "    # return the count divided by the\n", "    # denominator\n", "\n", "\n", "def students_t_stat(a,b):\n", "    # Subtract the mean of sample b\n", "    # from sample a (a-b)\n", "\n", "    # Calculate the pooled variance\n", "    # using the pooled_var function\n", "\n", "    # Calculate the denominator\n", "    # by multiplying the pooled variance\n", "    # by the sum of 1 over the length \n", "    # of both sample sizes\n", "\n", "    # return the mean difference divided by the denominator\n", "\n", "\n", "def students_ttest_2_samp(a,b):\n", "    # Calculate the t statistic\n", "    # using the students_t_stat function\n", "    \n", "    # Create a variable called \n", "    # positive_t by passing t into\n", "    # np.abs\n", "    \n", "    # Create a dof variable \n", "    # (stands for degrees of freedom)\n", "    # that adds the length of both samples\n", "    # and subtracts 2\n", "    \n", "    # Create a data_below_t variable\n", "    # that passes positive_t and dof\n", "    # into the stats.t.cdf function\n", "    \n", "    # Create a variable called data_above_t\n", "    # that subtracts data_below_t from 1\n", "    \n", "    # Create the two sides pvalue by\n", "    # multiplying data_above_t by 2\n", "\n", "    # return t and p\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's compare our code with scipy's `ttest_ind` function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T19:03:51.388268Z", "start_time": "2021-01-26T19:03:51.379682Z"}}, "outputs": [], "source": ["our_function = students_ttest_2_samp(experiment['Likes_Given_Exp'], control['Likes_Given_Con'])\n", "scipy_function = stats.ttest_ind(experiment['Likes_Given_Exp'], control['Likes_Given_Con'])\n", "\n", "print('Our function  : ', our_function)\n", "print('Scipy function: ', (scipy_function.statistic, scipy_function.pvalue))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Ttest Different Variance**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](img/t_welch.svg)\n", "![](img/welch_ddof.svg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T18:52:41.708019Z", "start_time": "2021-01-26T18:52:41.699100Z"}}, "outputs": [], "source": ["def welch_variables(sample):\n", "    # Calculate the mean of the sample\n", "    bar = sample.mean()\n", "    # Calculate the variance of the sample\n", "    var = sample.var(ddof=1)\n", "    # Calculate the size of the sample\n", "    n = len(sample)\n", "    # Calculate the degrees of freedom\n", "    dof = n - 1\n", "    \n", "    # return all of the calculations\n", "    return (bar, var, n, dof)\n", "\n", "def welch_dof(a,b):\n", "    # Calculate the required variables \n", "    # for both samples using the welch_variable function\n", "    a_bar, a_var, a_n, a_dof = welch_variables(a)\n", "    b_bar, b_var, b_n, b_dof = welch_variables(b)\n", "    \n", "    # Calculate the numerator for the degrees of freedom\n", "    dof_numerator = (a_var/a_n + b_var/b_n)**2\n", "    # Calculate the denominator for the degrees of freedom\n", "    dof_denominator = (a_var**2/(a_n**2 * a_dof) + b_var**2/(b_n**2 * b_dof))\n", "    # Divide the numerator by the denominator\n", "    dof = dof_numerator/dof_denominator\n", "    # return the degrees of freedom\n", "    return dof\n", "\n", "def welchs(a,b):\n", "    # Calculate the required variables \n", "    # for both samples using the welch_variable function\n", "    a_bar, a_var, a_n, a_dof = welch_variables(a)\n", "    b_bar, b_var, b_n, b_dof = welch_variables(b)\n", "    \n", "    # Subtracts the mean of sample b from \n", "    # the mean of sample a\n", "    t_numerator = a_bar - b_bar\n", "    # Divide the variance of both samples\n", "    # by the sample size for each sample\n", "    # Add the divisions together\n", "    # And calculate the square root\n", "    t_denominator = np.sqrt(a_var/a_n + b_var/b_n)\n", "    # Divide the numerator by the denominator\n", "    t = t_numerator/t_denominator\n", "    # Calculate the degrees of freedom with\n", "    # the welch_dof function\n", "    dof = welch_dof(a,b)\n", "    # Create a variable called \n", "    # positive_t by passing t into\n", "    # np.abs\n", "    positive_t = np.abs(t)\n", "    # Create a data_below_t variable\n", "    # that passes positive_t and dof\n", "    # into the stats.t.cdf function\n", "    data_below_t = stats.t.cdf(positive_t, dof)\n", "    # Create a variable called data_above_t\n", "    # that subtracts data_below_t from 1\n", "    data_above_t = 1-data_below_t\n", "    # Create the two sides pvalue by\n", "    # multiplying data_above_t by 2\n", "    p = data_above_t * 2\n", "    # return t and p\n", "    return t,p"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's compare our code with scipy's `ttest_ind` function with `equal_var` set to False."]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T22:39:26.766860Z", "start_time": "2021-01-26T22:39:26.761200Z"}}, "outputs": [], "source": ["our_function = welchs(experiment['Likes_Given_Exp'], control['Likes_Given_Con'])\n", "scipy_function = stats.ttest_ind(experiment['Likes_Given_Exp'], control['Likes_Given_Con'], equal_var=False)\n", "\n", "print('Our function  : ', our_function)\n", "print('Scipy function: ', (scipy_function.statistic, scipy_function.pvalue))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's take a look at the distributions for both samples"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T18:53:13.401025Z", "start_time": "2021-01-26T18:53:13.163960Z"}, "scrolled": false, "slideshow": {"slide_type": "-"}}, "outputs": [], "source": ["#warnings.filterwarnings(\"ignore\")\n", "sns.kdeplot(experiment['Likes_Given_Exp'], shade=True)\n", "sns.kdeplot(control['Likes_Given_Con'], shade=True);"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["### A Word of Caution\n", "\n", "This looks like a highly significant result! The low p-value tells us that the difference in sample statistics is likely not the result of chance. But we can infer that the new Instagram feature is relevant to the difference only if we can rule out other factors. Good experimental design would require that the control and experimental groups be constructed randomly. Let's check the \"Average Likes Given\" column to make sure we're on safe ground!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T17:36:33.259962Z", "start_time": "2021-01-26T17:36:33.255794Z"}}, "outputs": [], "source": ["effect_size_avg = cohen_d(experiment['Avg_Likes_Given_Exp'], control['Avg_Likes_Given_Con'])\n", "effect_size_avg"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T19:07:49.408471Z", "start_time": "2021-01-26T19:07:49.404985Z"}}, "outputs": [], "source": ["print(experiment['Avg_Likes_Given_Exp'].var())\n", "print(control['Avg_Likes_Given_Con'].var())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2021-01-26T19:07:16.103382Z", "start_time": "2021-01-26T19:07:16.098666Z"}}, "outputs": [], "source": ["stats.ttest_ind(experiment['Avg_Likes_Given_Exp'], control['Avg_Likes_Given_Con'], equal_var=False)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Indeed, this difference is not so significant. So we have good reason to think that there is a very significant difference between the two groups that cannot be explained by a difference in average likes between the two groups."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5. Limitations of Cohen's $d$\n", "\n", "Remember the story we told back in Part 1? It went like this: For a given effect size, we can shrink our $p$-value by increasing our sample size or inflate our $p$-value by decreasing our sample size. But the trouble with this story is that it assumes that effect size is independent of sample size. This, at least if we are measuring effect size in terms of Cohen's $d$, is false, since that metric depends on the standard deviation, which, in turn, depends on sample size.\n", "\n", "Of course, if we had access to the population parameters, then we could calculate Cohen's $d$ on those and this worry would effectively vanish. But in practice we do not have access to these, and so we are in the familiar position of estimating these based on samples.\n", "\n", "[Here](https://garstats.wordpress.com/2018/04/04/dbias/) is a nice resource on this problem.\n", "\n", "The basic upshot is this: Cohen's $d$, when calculated on samples, tends to **overestimate** the true effect size of a difference between populations (because population standard deviations are often larger than sample standard deviations), **especially** for small sample sizes. So: If your sample sizes are relatively small, take Cohen's $d$ with a grain of salt\u2013\u2013or use some more robust metric of effect size.\n", "\n", "For more on this, see [here](https://www.statisticssolutions.com/statistical-analyses-effect-size/) and [here](https://garstats.wordpress.com/2016/05/02/robust-effect-sizes-for-2-independent-groups/)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Appendix: Free Online Power Calculator\n", "\n", "Online Power Calculator: https://clincalc.com/Stats/Power.aspx"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": false, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "218.426px"}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}